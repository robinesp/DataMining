{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 3\n",
    "\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as).\n",
    "\n",
    " **Deadline:** Thursday, March 29,  2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "\n",
    "import sklearn.decomposition as deco\n",
    "import sklearn.manifold as manifold\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "#InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Isomap (5 Points, 1+2+2)\n",
    "\n",
    "Apply PCA and Isomap to images of handwritten digits (see below). You may use sklearn.decomposition and sklearn.manifold.\n",
    "\n",
    "### a)  \n",
    "Compute the first two components of the data using PCA. Make a scatter plot of the data in the first two components of PCA indicating class with color.\n",
    "\n",
    "### b)\n",
    " Compute an Isomap embedding with two components with nr_neighbors={5, 50, N-1} (three separate embeddings).\n",
    " For each of the Isomap embeddings, apply the function \"align\" (see below) with \"ref_data\" as your computed pca embedding and \"data\" as the isomap embedding. Show a scatter plot of each of the aligned isomap embeddings.\n",
    " \n",
    "### c)\n",
    "\n",
    "Visually compare how well the classes are separated in the different scatter plots. What is the effect of changing the number of neighbors on the score computed in the alignment function? What does it mean if the score is zero? When do you expect the score to become zero and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "N=len(X)\n",
    "\n",
    "# Align a data set with a reference data set minimizing l_1 error\n",
    "# Returns aligned data set and alignment error\n",
    "def align(ref_data, data):\n",
    "    \n",
    "    transformations = np.asarray([\n",
    "        [[0,1],[1,0]], \n",
    "        [[0,-1],[1,0]], \n",
    "        [[0,1],[-1,0]], \n",
    "        [[0,-1],[-1,0]], \n",
    "        [[1,0],[0,1]], \n",
    "        [[1,0],[0,-1]], \n",
    "        [[-1,0],[0,1]], \n",
    "        [[-1,0],[0,-1]] \n",
    "    ])\n",
    "    \n",
    "    score = []\n",
    "    for i in range(0,8):\n",
    "        transf_data =   np.matmul(data, transformations[i])\n",
    "        score.append(np.linalg.norm( transf_data - ref_data, ord=1) )\n",
    "        \n",
    "    idx = np.argmin(score)\n",
    "    transf_data = np.matmul(data,transformations[idx])\n",
    "    \n",
    "    print(\"Aligned the data sets. Score is {0:10.1f}  \".format(score[idx]))\n",
    "    \n",
    "    return transf_data, score[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Multidimensional Scaling (6 Points, 1+2+2+1)\n",
    "\n",
    "Show that for mean-centered data sets we can recover inner products using\n",
    "pairwise distance information only. This is used by the isomap embedding algorithm.\n",
    "\n",
    "We are given all squared pairwise distances of an otherwise unknown\n",
    "point set ${\\bf p}_1,\\dots, {\\bf p}_n \\in \\mathbb{R}^d$, i.e., we are given \n",
    "for all $1 \\leq i,j \\leq n$ the values\n",
    "\n",
    "$$  d_{ij} = \\|{\\bf p}_i - {\\bf p}_j\\|^2. $$\n",
    "\n",
    "We assume that the point set is mean-centered, that is \n",
    "\n",
    "$$ \\sum_{i=1}^{n} {\\bf p}_i = \\vec{{\\bf 0}}.$$\n",
    "\n",
    "(where $\\vec{{\\bf 0}}$ is the vector of zeros)\n",
    "\n",
    "\n",
    "In the following, $\\langle {\\bf p}_i , {\\bf p}_j \\rangle$ denotes the inner \n",
    "product of ${\\bf p}_i$ and ${\\bf p}_j$. \n",
    "Prove that the following holds true for mean-centered point sets: \n",
    "\n",
    "$$-2 \\langle {\\bf p}_i , {\\bf p}_j \\rangle = \n",
    "d_{ij}\n",
    "- \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n}   \n",
    "-  \\sum_{\\ell=1}^{n} \\frac{d_{j\\ell}}{n} \n",
    "+ \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{n^2} \n",
    "$$\n",
    "\n",
    "You may use the following steps in your derivation.\n",
    "\n",
    "### a) \n",
    "Expand  $d_{ij}$ to yield an expression of $\\langle {\\bf p}_i, {\\bf p}_j \\rangle$, $\\|{\\bf p}_i\\|^2$ and $\\|{\\bf p}_j\\|^2$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$$d_{ij}=\\|{\\bf p}_i - {\\bf p}_j\\|^2=\\langle {\\bf p}_i-{\\bf p}_j, {\\bf p}_i-{\\bf p}_j \\rangle=\\langle {\\bf p}_i, {\\bf p}_i \\rangle-\\langle {\\bf p}_i, {\\bf p}_j \\rangle-\\langle {\\bf p}_j, {\\bf p}_i \\rangle+\\langle {\\bf p}_j, {\\bf p}_j \\rangle=\\|{\\bf p}_i\\|^2-2\\langle {\\bf p}_i, {\\bf p}_j \\rangle+\\|{\\bf p}_j\\|^2$$\n",
    "\n",
    "\n",
    "\n",
    "### b) \n",
    "Show that the following holds for any $ {\\bf q} \\in \\mathbb{R}^d$:\n",
    "$$ \\sum_{1 \\leq i \\leq n} \\langle {\\bf p}_i , {\\bf q} \\rangle =   0$$\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "$ \\sum_{i=1}^{n} {\\bf p}_i = \\vec{{\\bf 0}}$  implies that for all $1 \\leq j \\leq d$ we will have $ \\sum_{i=1}^{n} {\\mbox p}_{ij} = 0$.\n",
    "\n",
    "Therefore, for any $ {\\bf q} \\in \\mathbb{R}^d$:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\langle {\\bf p}_i , {\\bf q} \\rangle = \\sum_{i=1}^{n} \\sum_{j=1}^{d} ( {\\mbox p}_{ij} \\cdot {\\mbox q}_j ) =  \\sum_{j=1}^{d} ( {\\mbox q}_j \\cdot \\sum_{i=1}^{n} {\\mbox p}_{ij} ) = \\sum_{j=1}^{d} ( {\\mbox q}_j \\cdot 0 ) = 0 $$\n",
    "\n",
    "\n",
    "\n",
    "### c) \n",
    "Prove that \n",
    "$$ \\|{\\bf p}_i\\|^2 = \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2}$$\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "We will prove this point starting from the right-hand side of the equation.\n",
    "\n",
    "$$ \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2} = $$\n",
    "\n",
    "$$ = \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_i - {\\bf p}_\\ell\\|^2}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_k - {\\bf p}_\\ell\\|^2}{2n^2} = $$\n",
    "\n",
    "$$ = \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_i\\|^2-2\\langle {\\bf p}_i , {\\bf p_\\ell} \\rangle+\\|{\\bf p}_\\ell\\|^2}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_k\\|^2-2\\langle {\\bf p}_k , {\\bf p_\\ell} \\rangle+\\|{\\bf p}_\\ell\\|^2}{2n^2} $$ \n",
    "\n",
    "Using **b)**, we can ignore the inner products:\n",
    "\n",
    "$$ = \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_i\\|^2+\\|{\\bf p}_\\ell\\|^2}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_k\\|^2+\\|{\\bf p}_\\ell\\|^2}{2n^2} = $$ \n",
    "\n",
    "$$ = \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_i\\|^2}{n} + \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_\\ell\\|^2}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_k\\|^2}{2n^2} -\\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\|{\\bf p}_\\ell\\|^2}{2n^2} = $$ \n",
    "\n",
    "$$ = n \\cdot \\frac{\\|{\\bf p}_i\\|^2}{n} + \\sum_{\\ell=1}^{n} \\frac{\\langle {\\bf p_\\ell} , {\\bf p_\\ell} \\rangle}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\langle {\\bf p}_k , {\\bf p}_k \\rangle}{2n^2} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{\\langle {\\bf p_\\ell} , {\\bf p_\\ell} \\rangle}{2n^2} = $$ \n",
    "\n",
    "We simplify again using **b)**:\n",
    "\n",
    "$$ = n \\cdot \\frac{\\|{\\bf p}_i\\|^2}{n} = \\|{\\bf p}_i\\|^2 $$ \n",
    "\n",
    "\n",
    "### d) \n",
    "Combine the steps in your proof. \n",
    "\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From **a)**, we know that $d_{ij} = \\|{\\bf p}_i\\|^2-2\\langle {\\bf p}_i, {\\bf p}_j \\rangle+\\|{\\bf p}_j\\|^2$ . Therefore:\n",
    "\n",
    "\n",
    "$$ -2\\langle {\\bf p}_i, {\\bf p}_j \\rangle = d_{ij} - \\|{\\bf p}_i\\|^2 - \\|{\\bf p}_j\\|^2 = d_{ij} - ( \\|{\\bf p}_i\\|^2 + \\|{\\bf p}_j\\|^2 ) $$\n",
    "\n",
    "From **c)**, we know that:\n",
    "\n",
    "$$ \\|{\\bf p}_i\\|^2 = \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2}$$\n",
    "\n",
    "Similarly, for $\\|{\\bf p}_j\\|^2$ we obtain:\n",
    "\n",
    "$$ \\|{\\bf p}_j\\|^2 = \\sum_{\\ell=1}^{n} \\frac{d_{j\\ell}}{n} - \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ \\|{\\bf p}_i\\|^2 + \\|{\\bf p}_j\\|^2 = \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n} + \\sum_{\\ell=1}^{n} \\frac{d_{j\\ell}}{n} - 2 \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{2n^2} $$\n",
    "\n",
    "This proves that: $$-2 \\langle {\\bf p}_i , {\\bf p}_j \\rangle = \n",
    "d_{ij}\n",
    "- \\sum_{\\ell=1}^{n} \\frac{d_{i\\ell}}{n}   \n",
    "-  \\sum_{\\ell=1}^{n} \\frac{d_{j\\ell}}{n} \n",
    "+ \\sum_{k=1}^{n}~ \\sum_{\\ell=1}^{n} \\frac{d_{k\\ell}}{n^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality-sensitive hashing (4 Points, 2+1+1)\n",
    "\n",
    "### a) \n",
    "\n",
    "Prove that if the Jaccard Similarity of two sets is $0$, then minhashing always gives a correct estimate of the Jaccard similarity\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Minhashing is computed by using a characteristic matrix with indicator vectors as columns: $h(S_i)$ is the index of the first row from the top, in the column $S_i$, which has a $1$. Therefore, we can compute minhashing has the probabilty of two sets of having the first $1$ in the same row, which is the number of $(1,1)$ rows over the total number of $(1,1)$, $(0,1)$ and $(1,0)$ rows. In this case the probabilty is exactly $0$:\n",
    "\n",
    "$$ Pr~[h(S_1) = h(S_2)] = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} = sim_J(S_1, S_2) = 0  $$\n",
    "\n",
    "We want to prove that minhashing gives in this case a correct estimate of the Jaccard similarity. Let $ h_1, ..., h_m $ be a number of Jaccard-Similarity-hashfunctions sampled at random. The estimate will be computed as:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^m I~( h_i~(S_1) == h_i~(S_2) ) $$\n",
    "\n",
    "Where $I(true):=1$ and $I(false):=0$. Since the two sets $S_1$ and $S_2$ are disjoint:\n",
    "\n",
    "\n",
    "$$  for ~ i = 1, ..., m  ~~~~~~  Pr~[h_i(S_1) = h_i(S_2)] = sim_J(S_1, S_2) = 0 $$\n",
    "\n",
    "And therefore:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^m I~( h_i~(S_1) == h_i~(S_2) ) = \\frac{1}{m} \\sum_{i=1}^m I~( false ) = \\frac{1}{m} \\sum_{i=1}^m 0 = 0 $$\n",
    "\n",
    "\n",
    "### b) \n",
    "Let $H$ be a family of $(d_1,d_2,p_1,p_2)$-locality-sensitive hash functions.\n",
    "Assume that $p_2=0$ and assume we have a total numer of $m$ hash\n",
    "functions from this family available.  Which combination of AND-constructions\n",
    "and OR-constructions should we use to maximally amplify the hash family?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "//TODO\n",
    "\n",
    "\n",
    "### (c) \n",
    "\n",
    "Let $H$ be a family of $(d_1,d_2,p_1,p_2)$-locality-sensitive hash functions.\n",
    "Assume that $p_2=\\frac{1}{n}$ and assume we have $n$ data points $\\bf P$\n",
    "which are stored in a hash table using a randomly chosen function $h$ from $H$.\n",
    "Given a query point $\\bf q$, we retrieve the points in the hash bucket with index $h(\\bf q)$ to search \n",
    "for a point which has small distance to $\\bf q$. \n",
    "Let $X$ be a random variable that is equal to the size of the set \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\{{\\bf p \\in P}~:~ h({\\bf p})=h({\\bf q}) ~\\wedge~ d({\\bf p,q}) \\geq d_2\\right\\}\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which consists of the false positives of this query.\n",
    "Derive the expected number of false-positives $E\\left[ X \\right]$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Let us define an indicator random variable $ Y_i $ , which returns $1$ if the point  $ {\\bf p_i} \\in {\\bf P } $ becomes a false-positive in the query with $\\bf q $  and $0$ if not. Then we can define $E\\left[ X \\right]$ as:\n",
    "\n",
    "$$ E\\left[ X \\right] = E\\left[ \\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n E\\left[ Y_i \\right] $$\n",
    "\n",
    "\n",
    "We can evaluate the expected value of $ Y_i $ as:\n",
    "\n",
    "$$ E\\left[ Y_i \\right] = 1 \\cdot Pr~[~ {\\bf p_i} \\text{ is a false positive in respect to } {\\bf q}~] = $$\n",
    "\n",
    "$$ = Pr~[~ h({\\bf p_i})=h({\\bf q}) ~\\wedge~ d({\\bf p_i,q}) \\geq d_2 ~] = $$\n",
    "\n",
    "$$ = Pr~[~ d({\\bf p_i,q}) \\geq d_2 ~] \\cdot Pr~[~ h({\\bf p_i})=h({\\bf q}) ~~|~~ d({\\bf p_i,q}) \\geq d_2 ~] $$\n",
    "\n",
    "\n",
    "Since $Pr~[~ d({\\bf p_i,q}) \\geq d_2 ~] <= 1$:\n",
    "\n",
    "$$ Pr~[~ d({\\bf p_i,q}) \\geq d_2 ~] \\cdot Pr~[~ h({\\bf p_i})=h({\\bf q}) ~~|~~ d({\\bf p_i,q}) \\geq d_2 ~] <= Pr~[~ h({\\bf p_i})=h({\\bf q}) ~~|~~ d({\\bf p_i,q}) \\geq d_2 ~] $$\n",
    "\n",
    "\n",
    "Since $h$ belongs to the family $H$, we know that:\n",
    "\n",
    "$$ Pr~[~ h({\\bf p_i})=h({\\bf q}) ~~|~~ d({\\bf p_i,q}) \\geq d_2 ~] <= p_2 $$\n",
    "\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ E\\left[ Y_i \\right] = Pr~[~ d({\\bf p_i,q}) \\geq d_2 ~] \\cdot Pr~[~ h({\\bf p_i})=h({\\bf q}) ~~|~~ d({\\bf p_i,q}) \\geq d_2 ~] <= p_2 $$\n",
    "\n",
    "\n",
    "We can conclude that the expected number of false positives is at most one:\n",
    "\n",
    "$$ E\\left[ X \\right] = \\sum_{i=1}^n E\\left[ Y_i \\right] <= \\sum_{i=1}^n p_2 = n \\cdot p_2 = \\frac{n}{n} = 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
